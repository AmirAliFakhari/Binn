{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 150, 150\n",
    "N_t = 77\n",
    "L_left, L_right = 0, 4380\n",
    "N_point = nx * ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3\n",
    "\n",
    "\n",
    "def get_dataset(experiment_idx):\n",
    "    print(f\"Processing dataset {j}...\")\n",
    "\n",
    "\n",
    "    U_total = np.zeros((N_point * N_t, 1))\n",
    "    X_total = np.zeros((N_point * N_t, 1))\n",
    "    Y_total = np.zeros((N_point * N_t, 1))\n",
    "    T_total = np.zeros((N_point * N_t, 1))\n",
    "\n",
    "    # Generate the grid for x, y, and time (t)\n",
    "    for i in range(N_t):\n",
    "        x = np.linspace(L_left, L_right, nx)\n",
    "        y = np.linspace(L_left, L_right, ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        T = np.full((N_point, 1), i * 20)\n",
    "\n",
    "        # Determine the slice of indices corresponding to time step i\n",
    "        start_idx = i * N_point\n",
    "        end_idx = (i + 1) * N_point\n",
    "\n",
    "        # Reshape X, Y to column vectors and assign\n",
    "        X_total[start_idx:end_idx, 0] = X.reshape(-1)\n",
    "        Y_total[start_idx:end_idx, 0] = Y.reshape(-1)\n",
    "        T_total[start_idx:end_idx, 0] = T.reshape(-1)\n",
    "\n",
    "    # Define input and output file paths\n",
    "    input_file = f\"./Data/U_total_{j}.mat\"\n",
    "    output_file = f\"./MeshData/TriangularMesh/U_xy{j}.mat\"\n",
    "    if j in [1, 2, 5, 6]:\n",
    "        output_file = f\"./MeshData/CircularMesh/U_xy{j}.mat\"\n",
    "\n",
    "    # Load simulation data from the .mat file\n",
    "    U = sio.loadmat(input_file)\n",
    "    # Convert to NumPy array, ensuring shape matches (N_point*N_t, 1)\n",
    "    U_data = np.array(U[f\"U_{j}\"][: N_point * N_t, 0])[:, None]\n",
    "\n",
    "    # Combine X, Y, T, and U into a single NumPy array\n",
    "    U_xy_np = np.hstack([X_total, Y_total, T_total, U_data])\n",
    "\n",
    "    # Optionally convert to PyTorch tensors\n",
    "    U_xy_torch = torch.from_numpy(U_xy_np.astype(np.float32))\n",
    "\n",
    "\n",
    "    return U_xy_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 3...\n",
      "TriangularMesh - Training Data Shape: torch.Size([1386000, 4])\n",
      "TriangularMesh - Testing Data Shape: torch.Size([346500, 4])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data: torch.Tensor):\n",
    "    \"\"\"\n",
    "    data: A torch.Tensor of shape (N, 4) containing [x, y, t, u].\n",
    "    This function normalizes each column to [0, 1] range.\n",
    "    Returns:\n",
    "        data: normalized in-place\n",
    "        data_bounds: dictionary of min/max for each column\n",
    "    \"\"\"\n",
    "    x_min, x_max = data[:, 0].min(), data[:, 0].max()\n",
    "    y_min, y_max = data[:, 1].min(), data[:, 1].max()\n",
    "    t_min, t_max = data[:, 2].min(), data[:, 2].max()\n",
    "    u_min, u_max = data[:, 3].min(), data[:, 3].max()\n",
    "\n",
    "    data[:, 0] = (data[:, 0] - x_min) / (x_max - x_min)\n",
    "    data[:, 1] = (data[:, 1] - y_min) / (y_max - y_min)\n",
    "    data[:, 2] = (data[:, 2] - t_min) / (t_max - t_min)\n",
    "    data[:, 3] = (data[:, 3] - u_min) / (u_max - u_min)\n",
    "\n",
    "    # Store bounds in a dictionary (convert Tensors to float)\n",
    "    data_bounds = {\n",
    "        \"x_min\": x_min.item(),\n",
    "        \"x_max\": x_max.item(),\n",
    "        \"y_min\": y_min.item(),\n",
    "        \"y_max\": y_max.item(),\n",
    "        \"t_min\": t_min.item(),\n",
    "        \"t_max\": t_max.item(),\n",
    "        \"u_min\": u_min.item(),\n",
    "        \"u_max\": u_max.item(),\n",
    "    }\n",
    "\n",
    "    return data, data_bounds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def invert_normalization(data: torch.Tensor, data_bound: dict, feature: str):\n",
    "    \"\"\"\n",
    "    Invert normalization for a given feature.\n",
    "    data: a torch.Tensor for the feature, in [0,1] range.\n",
    "    data_bound: dictionary with min/max values.\n",
    "    feature: one of 'x', 'y', 't', or 'u'.\n",
    "    \"\"\"\n",
    "    feature_min = data_bound[f\"{feature}_min\"]\n",
    "    feature_max = data_bound[f\"{feature}_max\"]\n",
    "\n",
    "    return data * (feature_max - feature_min) + feature_min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_and_preprocess_dataset(mesh_type: str, j: int):\n",
    "    \"\"\"\n",
    "    1) Loads dataset via get_dataset(j)\n",
    "    2) Splits into train/test\n",
    "    3) Normalizes each subset\n",
    "    4) Returns x_train, x_test, y_train, y_test, ... plus the test_min_max dictionary\n",
    "    \"\"\"\n",
    "    data = get_dataset(j)  # shape (N, 4), presumably on CPU right now\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Convert to NumPy for train_test_split (which only works on CPU), then back to Torch\n",
    "    data_np = data.cpu().numpy()  # move temporarily to CPU for NumPy ops\n",
    "    train_data_np, test_data_np = train_test_split(\n",
    "        data_np, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert back to torch Tensors on CPU\n",
    "    train_data_cpu = torch.from_numpy(train_data_np)\n",
    "    test_data_cpu = torch.from_numpy(test_data_np)\n",
    "\n",
    "    train_data = train_data_cpu.to(device)\n",
    "    test_data = test_data_cpu.to(device)\n",
    "\n",
    "    train_data, _ = preprocess_data(train_data)\n",
    "    test_data, test_min_max = preprocess_data(test_data)\n",
    "\n",
    "    x_train, y_train, t_train, u_train = (\n",
    "        train_data[:, 0],\n",
    "        train_data[:, 1],\n",
    "        train_data[:, 2],\n",
    "        train_data[:, 3],\n",
    "    )\n",
    "    x_test, y_test, t_test, u_test = (\n",
    "        test_data[:, 0],\n",
    "        test_data[:, 1],\n",
    "        test_data[:, 2],\n",
    "        test_data[:, 3],\n",
    "    )\n",
    "\n",
    "    # Print shapes\n",
    "    if mesh_type == \"circular\":\n",
    "        print(f\"CircularMesh - Training Data Shape: {train_data.shape}\")\n",
    "        print(f\"CircularMesh - Testing Data Shape: {test_data.shape}\")\n",
    "    else:\n",
    "        print(f\"TriangularMesh - Training Data Shape: {train_data.shape}\")\n",
    "        print(f\"TriangularMesh - Testing Data Shape: {test_data.shape}\")\n",
    "\n",
    "    return (\n",
    "        x_train,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        t_train,\n",
    "        t_test,\n",
    "        u_train,\n",
    "        u_test,\n",
    "        test_min_max,\n",
    "    )\n",
    "\n",
    "\n",
    "if j in [1, 2, 5, 6]:\n",
    "    mesh_type = \"circular\"\n",
    "else:\n",
    "    mesh_type = \"triangular\"\n",
    "\n",
    "x_train, x_val, y_train, y_val, t_train, t_val, u_train, u_val, test_min_max = (\n",
    "    split_and_preprocess_dataset(mesh_type, j)\n",
    ")\n",
    "\n",
    "x_train = torch.cat(\n",
    "    (x_train.unsqueeze(1), y_train.unsqueeze(1), t_train.unsqueeze(1)), dim=1\n",
    ")\n",
    "y_train = u_train\n",
    "\n",
    "x_val = torch.cat((x_val.unsqueeze(1), y_val.unsqueeze(1), t_val.unsqueeze(1)), dim=1)\n",
    "y_val = u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xyt):\n",
    "        return self.net(xyt)\n",
    "\n",
    "\n",
    "class D_Net(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softplus()) \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, u):\n",
    "        return self.net(u)\n",
    "\n",
    "\n",
    "class G_Net(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, u):\n",
    "        return self.net(u)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_aic_bic(loss, num_params, num_data_points):\n",
    "    \"\"\"\n",
    "    Computes the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC).\n",
    "    \n",
    "    Args:\n",
    "        loss (float): Negative log-likelihood or loss value.\n",
    "        num_params (int): Number of parameters in the model.\n",
    "        num_data_points (int): Number of data points used.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: AIC, BIC values.\n",
    "    \"\"\"\n",
    "    aic = 2 * num_params + 2 * loss\n",
    "    bic = num_params * np.log(num_data_points) + 2 * loss\n",
    "    return aic, bic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binn(u_net, D_net, G_net, train_loader, val_loader, epochs=10):\n",
    "    optimizer = optim.Adam(\n",
    "        list(u_net.parameters()) + list(D_net.parameters()) + list(G_net.parameters()),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    aic_values = []\n",
    "    bic_values = []\n",
    "\n",
    "    num_params = sum(p.numel() for p in u_net.parameters()) + \\\n",
    "                 sum(p.numel() for p in D_net.parameters()) + \\\n",
    "                 sum(p.numel() for p in G_net.parameters())\n",
    "    num_data_points = len(train_loader.dataset)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        u_net.train()\n",
    "        D_net.train()\n",
    "        G_net.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for x_batch, u_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} - Training\", leave=False):\n",
    "            x_batch = x_batch.to(device)\n",
    "            u_batch = u_batch.to(device)\n",
    "            x_batch = x_batch.requires_grad_(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            u_pred = u_net(x_batch)\n",
    "            loss_data = torch.mean((u_pred - u_batch) ** 2)\n",
    "\n",
    "            D = D_net(u_pred)\n",
    "            G = G_net(u_pred)\n",
    "\n",
    "            grad_u = torch.autograd.grad(\n",
    "                u_pred, x_batch, grad_outputs=torch.ones_like(u_pred), create_graph=True\n",
    "            )[0]\n",
    "            du_dx, du_dy, du_dt = grad_u[:, 0:1], grad_u[:, 1:2], grad_u[:, 2:3]\n",
    "            term_x = D * du_dx\n",
    "            term_y = D * du_dy\n",
    "            d_term_x = torch.autograd.grad(\n",
    "                term_x, x_batch, grad_outputs=torch.ones_like(term_x), create_graph=True\n",
    "            )[0][:, 0:1]\n",
    "            d_term_y = torch.autograd.grad(\n",
    "                term_y, x_batch, grad_outputs=torch.ones_like(term_y), create_graph=True\n",
    "            )[0][:, 1:2]\n",
    "            pde_residual = du_dt - (d_term_x + d_term_y + G * u_pred)\n",
    "            loss_pde = torch.mean(pde_residual**2)\n",
    "\n",
    "            loss_total = loss_data + loss_pde\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_total.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, u_val in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} - Validation\", leave=False):\n",
    "                x_val = x_val.to(device)\n",
    "                u_val = u_val.to(device)\n",
    "                u_pred = u_net(x_val)\n",
    "                val_loss += torch.mean((u_pred - u_val) ** 2).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        \n",
    "        aic, bic = compute_aic_bic(avg_train_loss, num_params, num_data_points)\n",
    "        aic_values.append(aic)\n",
    "        bic_values.append(bic)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | AIC: {aic:.2f} | BIC: {bic:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), aic_values, label=\"AIC\", color=\"blue\")\n",
    "    plt.plot(range(1, epochs + 1), bic_values, label=\"BIC\", color=\"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Criterion Value\")\n",
    "    plt.title(\"AIC and BIC Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "u_net = U_Net(input_dim=3).to(device)  # 3 input features (x, y, t)\n",
    "D_net = D_Net(input_dim=1).to(device)  # 1 input feature (u)\n",
    "G_net = G_Net(input_dim=1).to(device)  # 1 input feature (u)\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "train_binn(u_net, D_net, G_net, train_loader, val_loader, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
