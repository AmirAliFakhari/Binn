{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 150, 150\n",
    "N_t = 77\n",
    "L_left, L_right = 0, 4380\n",
    "N_point = nx * ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 3\n",
    "\n",
    "\n",
    "def get_dataset(experiment_idx):\n",
    "    print(f\"Processing dataset {j}...\")\n",
    "\n",
    "\n",
    "    U_total = np.zeros((N_point * N_t, 1))\n",
    "    X_total = np.zeros((N_point * N_t, 1))\n",
    "    Y_total = np.zeros((N_point * N_t, 1))\n",
    "    T_total = np.zeros((N_point * N_t, 1))\n",
    "\n",
    "    # Generate the grid for x, y, and time (t)\n",
    "    for i in range(N_t):\n",
    "        x = np.linspace(L_left, L_right, nx)\n",
    "        y = np.linspace(L_left, L_right, ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        T = np.full((N_point, 1), i * 20)\n",
    "\n",
    "        # Determine the slice of indices corresponding to time step i\n",
    "        start_idx = i * N_point\n",
    "        end_idx = (i + 1) * N_point\n",
    "\n",
    "        # Reshape X, Y to column vectors and assign\n",
    "        X_total[start_idx:end_idx, 0] = X.reshape(-1)\n",
    "        Y_total[start_idx:end_idx, 0] = Y.reshape(-1)\n",
    "        T_total[start_idx:end_idx, 0] = T.reshape(-1)\n",
    "\n",
    "    # Define input and output file paths\n",
    "    input_file = f\"./Data/U_total_{j}.mat\"\n",
    "    output_file = f\"./MeshData/TriangularMesh/U_xy{j}.mat\"\n",
    "    if j in [1, 2, 5, 6]:\n",
    "        output_file = f\"./MeshData/CircularMesh/U_xy{j}.mat\"\n",
    "\n",
    "    # Load simulation data from the .mat file\n",
    "    U = sio.loadmat(input_file)\n",
    "    # Convert to NumPy array, ensuring shape matches (N_point*N_t, 1)\n",
    "    U_data = np.array(U[f\"U_{j}\"][: N_point * N_t, 0])[:, None]\n",
    "\n",
    "    # Combine X, Y, T, and U into a single NumPy array\n",
    "    U_xy_np = np.hstack([X_total, Y_total, T_total, U_data])\n",
    "\n",
    "    # Optionally convert to PyTorch tensors\n",
    "    U_xy_torch = torch.from_numpy(U_xy_np.astype(np.float32))\n",
    "\n",
    "\n",
    "    return U_xy_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 3...\n",
      "TriangularMesh - Training Data Shape: torch.Size([1386000, 4])\n",
      "TriangularMesh - Testing Data Shape: torch.Size([346500, 4])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data: torch.Tensor):\n",
    "    \"\"\"\n",
    "    data: A torch.Tensor of shape (N, 4) containing [x, y, t, u].\n",
    "    This function normalizes each column to [0, 1] range.\n",
    "    Returns:\n",
    "        data: normalized in-place\n",
    "        data_bounds: dictionary of min/max for each column\n",
    "    \"\"\"\n",
    "    x_min, x_max = data[:, 0].min(), data[:, 0].max()\n",
    "    y_min, y_max = data[:, 1].min(), data[:, 1].max()\n",
    "    t_min, t_max = data[:, 2].min(), data[:, 2].max()\n",
    "    u_min, u_max = data[:, 3].min(), data[:, 3].max()\n",
    "\n",
    "    data[:, 0] = (data[:, 0] - x_min) / (x_max - x_min)\n",
    "    data[:, 1] = (data[:, 1] - y_min) / (y_max - y_min)\n",
    "    data[:, 2] = (data[:, 2] - t_min) / (t_max - t_min)\n",
    "    data[:, 3] = (data[:, 3] - u_min) / (u_max - u_min)\n",
    "\n",
    "    # Store bounds in a dictionary (convert Tensors to float)\n",
    "    data_bounds = {\n",
    "        \"x_min\": x_min.item(),\n",
    "        \"x_max\": x_max.item(),\n",
    "        \"y_min\": y_min.item(),\n",
    "        \"y_max\": y_max.item(),\n",
    "        \"t_min\": t_min.item(),\n",
    "        \"t_max\": t_max.item(),\n",
    "        \"u_min\": u_min.item(),\n",
    "        \"u_max\": u_max.item(),\n",
    "    }\n",
    "\n",
    "    return data, data_bounds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def invert_normalization(data: torch.Tensor, data_bound: dict, feature: str):\n",
    "    \"\"\"\n",
    "    Invert normalization for a given feature.\n",
    "    data: a torch.Tensor for the feature, in [0,1] range.\n",
    "    data_bound: dictionary with min/max values.\n",
    "    feature: one of 'x', 'y', 't', or 'u'.\n",
    "    \"\"\"\n",
    "    feature_min = data_bound[f\"{feature}_min\"]\n",
    "    feature_max = data_bound[f\"{feature}_max\"]\n",
    "\n",
    "    return data * (feature_max - feature_min) + feature_min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_and_preprocess_dataset(mesh_type: str, j: int):\n",
    "    \"\"\"\n",
    "    1) Loads dataset via get_dataset(j)\n",
    "    2) Splits into train/test\n",
    "    3) Normalizes each subset\n",
    "    4) Returns x_train, x_test, y_train, y_test, ... plus the test_min_max dictionary\n",
    "    \"\"\"\n",
    "    data = get_dataset(j)  # shape (N, 4), presumably on CPU right now\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Convert to NumPy for train_test_split (which only works on CPU), then back to Torch\n",
    "    data_np = data.cpu().numpy()  # move temporarily to CPU for NumPy ops\n",
    "    train_data_np, test_data_np = train_test_split(\n",
    "        data_np, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert back to torch Tensors on CPU\n",
    "    train_data_cpu = torch.from_numpy(train_data_np)\n",
    "    test_data_cpu = torch.from_numpy(test_data_np)\n",
    "\n",
    "    train_data = train_data_cpu.to(device)\n",
    "    test_data = test_data_cpu.to(device)\n",
    "\n",
    "    train_data, _ = preprocess_data(train_data)\n",
    "    test_data, test_min_max = preprocess_data(test_data)\n",
    "\n",
    "    x_train, y_train, t_train, u_train = (\n",
    "        train_data[:, 0],\n",
    "        train_data[:, 1],\n",
    "        train_data[:, 2],\n",
    "        train_data[:, 3],\n",
    "    )\n",
    "    x_test, y_test, t_test, u_test = (\n",
    "        test_data[:, 0],\n",
    "        test_data[:, 1],\n",
    "        test_data[:, 2],\n",
    "        test_data[:, 3],\n",
    "    )\n",
    "\n",
    "    # Print shapes\n",
    "    if mesh_type == \"circular\":\n",
    "        print(f\"CircularMesh - Training Data Shape: {train_data.shape}\")\n",
    "        print(f\"CircularMesh - Testing Data Shape: {test_data.shape}\")\n",
    "    else:\n",
    "        print(f\"TriangularMesh - Training Data Shape: {train_data.shape}\")\n",
    "        print(f\"TriangularMesh - Testing Data Shape: {test_data.shape}\")\n",
    "\n",
    "    return (\n",
    "        x_train,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        t_train,\n",
    "        t_test,\n",
    "        u_train,\n",
    "        u_test,\n",
    "        test_min_max,\n",
    "    )\n",
    "\n",
    "\n",
    "if j in [1, 2, 5, 6]:\n",
    "    mesh_type = \"circular\"\n",
    "else:\n",
    "    mesh_type = \"triangular\"\n",
    "\n",
    "x_train, x_val, y_train, y_val, t_train, t_val, u_train, u_val, test_min_max = (\n",
    "    split_and_preprocess_dataset(mesh_type, j)\n",
    ")\n",
    "\n",
    "x_train = torch.cat(\n",
    "    (x_train.unsqueeze(1), y_train.unsqueeze(1), t_train.unsqueeze(1)), dim=1\n",
    ")\n",
    "y_train = u_train\n",
    "\n",
    "x_val = torch.cat((x_val.unsqueeze(1), y_val.unsqueeze(1), t_val.unsqueeze(1)), dim=1)\n",
    "y_val = u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xyt):\n",
    "        return self.net(xyt)\n",
    "\n",
    "\n",
    "class D_Net(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softplus()) \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, u):\n",
    "        return self.net(u)\n",
    "\n",
    "\n",
    "class G_Net(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, u):\n",
    "        return self.net(u)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_aic_bic(loss, num_params, num_data_points):\n",
    "    \"\"\"\n",
    "    Computes the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC).\n",
    "    \n",
    "    Args:\n",
    "        loss (float): Negative log-likelihood or loss value.\n",
    "        num_params (int): Number of parameters in the model.\n",
    "        num_data_points (int): Number of data points used.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: AIC, BIC values.\n",
    "    \"\"\"\n",
    "    aic = 2 * num_params + 2 * loss\n",
    "    bic = num_params * np.log(num_data_points) + 2 * loss\n",
    "    return aic, bic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binn(u_net, D_net, G_net, train_loader, val_loader, epochs=10):\n",
    "    optimizer = optim.Adam(\n",
    "        list(u_net.parameters()) + list(D_net.parameters()) + list(G_net.parameters()),\n",
    "        lr=1e-3,\n",
    "    )\n",
    "\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    aic_values = []\n",
    "    bic_values = []\n",
    "\n",
    "    num_params = sum(p.numel() for p in u_net.parameters()) + \\\n",
    "                 sum(p.numel() for p in D_net.parameters()) + \\\n",
    "                 sum(p.numel() for p in G_net.parameters())\n",
    "    num_data_points = len(train_loader.dataset)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        u_net.train()\n",
    "        D_net.train()\n",
    "        G_net.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for x_batch, u_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} - Training\", leave=False):\n",
    "            x_batch = x_batch.requires_grad_(True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            u_pred = u_net(x_batch)\n",
    "            loss_data = torch.mean((u_pred - u_batch) ** 2)\n",
    "\n",
    "            D = D_net(u_pred)\n",
    "            G = G_net(u_pred)\n",
    "\n",
    "            grad_u = torch.autograd.grad(\n",
    "                u_pred, x_batch, grad_outputs=torch.ones_like(u_pred), create_graph=True\n",
    "            )[0]\n",
    "            du_dx, du_dy, du_dt = grad_u[:, 0:1], grad_u[:, 1:2], grad_u[:, 2:3]\n",
    "            term_x = D * du_dx\n",
    "            term_y = D * du_dy\n",
    "            d_term_x = torch.autograd.grad(\n",
    "                term_x, x_batch, grad_outputs=torch.ones_like(term_x), create_graph=True\n",
    "            )[0][:, 0:1]\n",
    "            d_term_y = torch.autograd.grad(\n",
    "                term_y, x_batch, grad_outputs=torch.ones_like(term_y), create_graph=True\n",
    "            )[0][:, 1:2]\n",
    "            pde_residual = du_dt - (d_term_x + d_term_y + G * u_pred)\n",
    "            loss_pde = torch.mean(pde_residual**2)\n",
    "\n",
    "            loss_total = loss_data + loss_pde\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_total.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, u_val in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} - Validation\", leave=False):\n",
    "                u_pred = u_net(x_val)\n",
    "                val_loss += torch.mean((u_pred - u_val) ** 2).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        \n",
    "        aic, bic = compute_aic_bic(avg_train_loss, num_params, num_data_points)\n",
    "        aic_values.append(aic)\n",
    "        bic_values.append(bic)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | AIC: {aic:.2f} | BIC: {bic:.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs + 1), aic_values, label=\"AIC\", color=\"blue\")\n",
    "    plt.plot(range(1, epochs + 1), bic_values, label=\"BIC\", color=\"red\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Criterion Value\")\n",
    "    plt.title(\"AIC and BIC Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|          | 0/10829 [00:00<?, ?it/s]c:\\Users\\parsa\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrain_binn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 43\u001b[0m, in \u001b[0;36mtrain_binn\u001b[1;34m(u_net, D_net, G_net, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m term_y \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m*\u001b[39m du_dy\n\u001b[0;32m     40\u001b[0m d_term_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[0;32m     41\u001b[0m     term_x, x_batch, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(term_x), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     42\u001b[0m )[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 43\u001b[0m d_term_y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mterm_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     46\u001b[0m pde_residual \u001b[38;5;241m=\u001b[39m du_dt \u001b[38;5;241m-\u001b[39m (d_term_x \u001b[38;5;241m+\u001b[39m d_term_y \u001b[38;5;241m+\u001b[39m G \u001b[38;5;241m*\u001b[39m u_pred)\n\u001b[0;32m     47\u001b[0m loss_pde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(pde_residual\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\parsa\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\parsa\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "u_net = U_Net(input_dim=3).to(device)  # 3 input features (x, y, t)\n",
    "D_net = D_Net(input_dim=1).to(device)  # 1 input feature (u)\n",
    "G_net = G_Net(input_dim=1).to(device)  # 1 input feature (u)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "train_binn(u_net, D_net, G_net, train_loader, val_loader, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
